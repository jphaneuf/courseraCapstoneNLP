---
title: "Coursera/JHU NLP Milestone Report"
author: "joe phaneuf"
date: "December 16, 2015"
output: pdf_document
---
#Introduction

Load required libraries and load the English language datasets.
```{r message=FALSE}
library(tm)
library(quanteda)
library(RWeka)
enDir <- './final/en_US'
enDir <- './finalsub/en_US'
ec <- VCorpus(DirSource(enDir))#english corpus
```
#Preprocessing and Cleaning  
Clean and tokenize the dataset using MC_tokenizer from tm package
```{r message=FALSE}
ect <- tm_map(ec,MC_tokenizer)#english corpus tokenized
```
#Findings  
The corpus is converted to a Term Document Matrix to show frequency of discrete words contained in the Corpus.
```{r message=FALSE,results='hide'}
dtm <- tm_map(ect, PlainTextDocument)#prevent error on TermDocumentMatrix
dtm<- TermDocumentMatrix(dtm)
wordCount <- dtm$nrow
```
Found `r wordCount` distinct words  
A histogram can be constructed from the term document matrix to show the frequency of word counts
```{r message=FALSE}
hist(round(rowMeans(as.matrix(dtm))))
```
  
#Prediction  
After cleaning and processing the data to create a usable database of n-grams, a Shiny App will be constructed to reference user input against the n-gram database, using the most frequently occuring matching n-gram to predict the user's input.
